{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def generate_crossword_words():\n",
    "    # Download required NLTK data\n",
    "    nltk.download('words')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('brown')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    # Get all English words\n",
    "    word_list = set(words.words())\n",
    "    \n",
    "    # Function to check if word is suitable for crosswords\n",
    "    def is_suitable_word(word):\n",
    "        # Must be at least 3 letters\n",
    "        if len(word) < 3:\n",
    "            return False\n",
    "        \n",
    "        if len(word) > 11:\n",
    "            return False\n",
    "            \n",
    "        # Must be all letters (no numbers or special characters)\n",
    "        if not word.isalpha():\n",
    "            return False\n",
    "            \n",
    "        # Must be in common usage (using WordNet presence as a proxy)\n",
    "        if not wordnet.synsets(word):\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    # Filter initial word list\n",
    "    suitable_words = {word.lower() for word in word_list if is_suitable_word(word)}\n",
    "    \n",
    "    # Load Brown corpus for frequency analysis\n",
    "    from nltk.corpus import brown\n",
    "    brown_words = [word.lower() for word in brown.words() \n",
    "                   if word.lower() in suitable_words]\n",
    "    \n",
    "    # Get word frequencies\n",
    "    word_freq = Counter(brown_words)\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame.from_dict(word_freq, orient='index', \n",
    "                               columns=['frequency']).reset_index()\n",
    "    df.columns = ['word', 'frequency']\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df = df[~df['word'].isin(stop_words)]\n",
    "    \n",
    "    # Sort by frequency and take top 30,000\n",
    "    df = df.sort_values('frequency', ascending=False).head(30000)\n",
    "    \n",
    "    # Add word length column\n",
    "    df['length'] = df['word'].str.len()\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('crossword_words.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_crossword_words()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def generate_gemini_clue(word, model, style_prompt):\n",
    "    \"\"\"Generate clue using Gemini\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(f\"{style_prompt}. Use this style for the word {word}. Reply only with the clue. Do not include a number in brackets or emojis.\")\n",
    "        return response.text.strip().strip('\"')\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_anthropic_clue(word, client, style_prompt):\n",
    "    \"\"\"Generate clue using Anthropic's Claude\"\"\"\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{style_prompt}. Use this style for the word {word}. Reply only with the clue. Do not include a number in brackets of emojis.\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        content = message.content[0].text if isinstance(message.content, list) else message.content\n",
    "        return str(content).strip().strip('\"')\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_openai_clue(word, client, style_prompt):\n",
    "    \"\"\"Generate clue using OpenAI\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a crossword clue writer. Reply only with the clue.. Do not include a number in brackets or emojis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{style_prompt}. Use this style for the word {word}.\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().strip('\"')\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_style_prompt():\n",
    "    \"\"\"Randomly select a clue style based on specified probabilities\"\"\"\n",
    "    rand = random.random()\n",
    "    \n",
    "    if rand < 0.6:  # 60% easy clues\n",
    "        return (\"Create an easy but clever 2-4 word crossword clue. \"\n",
    "                \"The clue should be straightforward but engaging. \"\n",
    "                \"For example, 'Night bird' for OWL.\"), \"Easy\"\n",
    "    \n",
    "    elif rand < 0.75:  # 15% moderate clues\n",
    "        return (\"Create a moderately challenging 2-4 word crossword clue. \"\n",
    "                \"The clue should require some thought but not be obscure. \"\n",
    "                \"For example, 'Desert wanderer' for CAMEL.\"), \"Moderate\"\n",
    "    \n",
    "    elif rand < 0.85:  # 10% fill in blank\n",
    "        return (\"Create a fill-in-the-blank crossword clue using ___ notation. \"\n",
    "                \"For example, '___ and done' for the answer SAID. \"\n",
    "                \"Use exactly this format with the underscores. Only one word should be blank.\"), \"Fill-in-blank\"\n",
    "    \n",
    "    elif rand < 0.925:  # 7.5% wordplay with ?\n",
    "        return (\"Create an easy wordplay crossword clue that ends with a question mark. Ensure these clues are relatively easy while still being clever.\"\n",
    "                \"For example, 'Flying home?' for the answer NEST. \"\n",
    "                \"The question mark signals wordplay to the solver.\"), \"Wordplay\"\n",
    "    \n",
    "    else:  # 7.5% perhaps with examples\n",
    "        return (\"Create a crossword clue by using an example of the target word (if possible). Follow the clue with ,perhaps so it is clear it's an example. \"\n",
    "                \"For example, 'Fedoras, perhaps' for the answer HATS. \"\n",
    "                \"Use exactly this format with ', perhaps' at the end.\"), \"Perhaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_clues(word: str, clue_type: str, clues: Dict[str, str], anthropic_client) -> tuple[str, str, str]:\n",
    "    \"\"\"Have Claude evaluate and rank the clues\"\"\"\n",
    "    evaluation_prompt = f\"\"\"You are an expert crossword editor. Evaluate these crossword clues for the word \"{word}\".\n",
    "    Clue type: {clue_type}\n",
    "    \n",
    "    Clues to evaluate:\n",
    "    1. Gemini: {clues['Gemini']}\n",
    "    2. Claude: {clues['Claude']}\n",
    "    3. GPT-4o: {clues['GPT-4o']}\n",
    "    \n",
    "    Consider these criteria:\n",
    "    - Accuracy and appropriateness\n",
    "    - Adherence to clue type format\n",
    "    - Clarity and fairness\n",
    "    - Avoidance of the answer in the clue\n",
    "    - Following crossword conventions\n",
    "    \n",
    "    Rank these clues from best to worst and explain why the winner is best. If no clue is up to standard, explain why and state None as the winner. \n",
    "    Reply in this exact format:\n",
    "    WINNER: [model name]\n",
    "    WINNING_CLUE: [the winning clue]\n",
    "    EXPLANATION: [1-2 sentence explanation]\"\"\"\n",
    "\n",
    "    try:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        response = message.content[0].text if isinstance(message.content, list) else message.content\n",
    "        \n",
    "        # Parse response\n",
    "        lines = response.split('\\n')\n",
    "        winner = next(line for line in lines if line.startswith('WINNER:')).replace('WINNER:', '').strip()\n",
    "        winning_clue = next(line for line in lines if line.startswith('WINNING_CLUE:')).replace('WINNING_CLUE:', '').strip()\n",
    "        explanation = next(line for line in lines if line.startswith('EXPLANATION:')).replace('EXPLANATION:', '').strip()\n",
    "        \n",
    "        return winner, winning_clue, explanation\n",
    "    except Exception as e:\n",
    "        return \"Error\", \"Error in evaluation\", str(e)\n",
    "\n",
    "def generate_comparative_clues(df, start_from=0, num_words=10):\n",
    "    \"\"\"Generate and compare clues from all three models, with automated evaluation\"\"\"\n",
    "    # Initialize all clients\n",
    "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "    gemini_model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "    anthropic_client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "    openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Initialize results list based on checkpoint or fresh start\n",
    "    if start_from == 0:\n",
    "        results = []\n",
    "    else:\n",
    "        # Read checkpoint file and convert to list of dictionaries\n",
    "        checkpoint_df = pd.read_csv(f'model_clue_comparison_with_evaluation_checkpoint_{start_from}.csv')\n",
    "        # Drop any unnamed index columns that might have been created\n",
    "        unnamed_cols = [col for col in checkpoint_df.columns if 'Unnamed' in col]\n",
    "        checkpoint_df = checkpoint_df.drop(columns=unnamed_cols)\n",
    "        results = checkpoint_df.to_dict('records')\n",
    "\n",
    "    for i in tqdm(range(start_from+1, min(num_words, len(df)))):\n",
    "        word = df.iloc[i].word\n",
    "        \n",
    "        # Use same style for all models for fair comparison\n",
    "        style_prompt, style_type = get_style_prompt()\n",
    "        \n",
    "        try:\n",
    "            # Generate clues from all models\n",
    "            clues = {\n",
    "                'Gemini': generate_gemini_clue(word, gemini_model, style_prompt),\n",
    "                'Claude': generate_anthropic_clue(word, anthropic_client, style_prompt),\n",
    "                'GPT-4o': generate_openai_clue(word, openai_client, style_prompt)\n",
    "            }\n",
    "            \n",
    "            # Have Claude evaluate the clues\n",
    "            winner, winning_clue, explanation = evaluate_clues(word, style_type, clues, anthropic_client)\n",
    "            \n",
    "            results.append({\n",
    "                'Word': word,\n",
    "                'Clue Type': style_type,\n",
    "                'Gemini': clues['Gemini'],\n",
    "                'Claude': clues['Claude'],\n",
    "                'GPT-4o': clues['GPT-4o'],\n",
    "                'Winning Model': winner,\n",
    "                'Winning Clue': winning_clue,\n",
    "                'Explanation': explanation\n",
    "            })\n",
    "\n",
    "            # Save checkpoint at regular intervals or specific milestones\n",
    "            if (i % 1000 == 0):\n",
    "                results_df = pd.DataFrame(results)\n",
    "                results_df.to_csv(f'model_clue_comparison_with_evaluation_checkpoint_{i}.csv', index=False)\n",
    "                print(f\"Saved checkpoint at iteration {i}\")\n",
    "            \n",
    "            # Display progress update\n",
    "            if (i % 50 == 0) or (i == (start_from +1)):\n",
    "                results_df = pd.DataFrame(results)\n",
    "                print(results_df.tail())\n",
    "                results_df.to_csv(f'model_clue_comparison_with_evaluation_checkpoint_{i}.csv', index=False)\n",
    "                print(f\"Saved checkpoint at iteration {i}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating clues for word '{word}' at index {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert final results to DataFrame and save\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('model_clue_comparison_with_evaluation.csv', index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results_df = generate_comparative_clues(df, start_from=0, num_words=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossword_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
